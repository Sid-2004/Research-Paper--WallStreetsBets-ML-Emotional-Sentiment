{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmRjtGaHJc0gXfje3FZeqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sid-2004/Research-Paper--WallStreetsBets-ML-Emotional-Sentiment/blob/main/Research_Paper_FinTech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T0mOc3YHkTy"
      },
      "outputs": [],
      "source": [
        "!pip install praw yfinance transformers torch tensorflow keras-tcn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 0. Setup: installs & imports\n",
        "# ---------------------------\n",
        "!pip install praw transformers sentence-transformers yfinance pytorch-lightning pytorch-forecasting==0.10.4 torchinfo\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  # CPU fallback if GPU not available\n",
        "\n",
        "# Standard imports\n",
        "import os, re, time, math, json, datetime\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import yfinance as yf\n"
      ],
      "metadata": {
        "id": "SexGCbwGSQqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw yfinance transformers torch tensorflow keras-tcn\n",
        "import praw\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EuLyjHYHeYZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(client_id=\"gpXjQ2OtfofKUYW-rcMJrw\",\n",
        "client_secret=\"TfeCSLEoqahvSnikIFDYXdAavE7qww\",\n",
        "user_agent=\"my_wsb_scraper/0.1 by your_momsbob_is_mine\"\n",
        ")\n",
        "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
        "\n",
        "# Example: Get top 100 posts from this week\n",
        "posts = []\n",
        "for submission in subreddit.top(limit=100, time_filter=\"week\"):\n",
        "    posts.append({\n",
        "        \"title\": submission.title,\n",
        "        \"text\": submission.selftext,\n",
        "        \"score\": submission.score,\n",
        "        \"comments\": submission.num_comments\n",
        "    })"
      ],
      "metadata": {
        "id": "-XWvo_Ul-LVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(posts)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ls7WueU0A_cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "11OZIxLFBocE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"wsb_posts.csv\",index=False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Ail8jZwjB0Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "df=pd.read_csv(\"wsb_posts.csv\")\n",
        "\n",
        "def clean_text(text):\n",
        "  if not isinstance(text, str):\n",
        "    return \"\"\n",
        "  text = text.lower()                           # lowercase\n",
        "  text = re.sub(r\"http\\S+|www\\S+\", \"\", text)    # remove links\n",
        "  text = re.sub(r\"[^a-z\\s]\", \"\", text)          # remove punctuation/numbers\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()      # remove extra spaces\n",
        "  return text\n",
        "\n",
        "df[\"clean_text\"] = df[\"title\"].astype(str) + \" \" + df[\"text\"].astype(str)\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(clean_text)\n",
        "\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "e-Fzm7z0CUfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 1. Data Collection (using PRAW)\n",
        "# ---------------------------\n",
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your own Reddit API credentials (from https://www.reddit.com/prefs/apps)\n",
        "# Make sure to keep these secret!\n",
        "reddit = praw.Reddit(client_id=\"gpXjQ2OtfofKUYW-rcMJrw\",\n",
        "                     client_secret=\"TfeCSLEoqahvSnikIFDYXdAavE7qww\",\n",
        "                     user_agent=\"my_wsb_scraper/0.1 by your_momsbob_is_mine\")\n",
        "\n",
        "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
        "\n",
        "# Get a larger number of top posts from this week using PRAW\n",
        "posts = []\n",
        "# Increase the limit to get more posts, up to 1000\n",
        "for submission in subreddit.top(limit=1000, time_filter=\"week\"):\n",
        "    posts.append({\n",
        "        \"title\": submission.title,\n",
        "        \"text\": submission.selftext,\n",
        "        \"score\": submission.score,\n",
        "        \"comments\": submission.num_comments\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(posts)\n",
        "df.to_csv(\"wsb_posts_large.csv\", index=False)\n",
        "print(f\"Saved {len(df)} posts to wsb_posts_large.csv\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "pLwZGR-JDelQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d202cb0d"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the PRAW Reddit object and retrieve a smaller number of top posts from the specified subreddit, then store the relevant data in a DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81e23c91"
      },
      "source": [
        "reddit = praw.Reddit(client_id=\"gpXjQ2OtfofKUYW-rcMJrw\",\n",
        "client_secret=\"TfeCSLEoqahvSnikIFDYXdAavE7qww\",\n",
        "user_agent=\"my_wsb_scraper/0.1 by your_momsbob_is_mine\"\n",
        ")\n",
        "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
        "\n",
        "# Get a smaller number of top posts from this week\n",
        "posts = []\n",
        "for submission in subreddit.top(limit=50, time_filter=\"week\"):\n",
        "    posts.append({\n",
        "        \"title\": submission.title,\n",
        "        \"text\": submission.selftext,\n",
        "        \"score\": submission.score,\n",
        "        \"comments\": submission.num_comments\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(posts)\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76372b8c"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the clean_text function, concatenate the title and text columns, apply the cleaning function, and display the head of the dataframe to verify the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49216f9d"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  if not isinstance(text, str):\n",
        "    return \"\"\n",
        "  text = text.lower()                           # lowercase\n",
        "  text = re.sub(r\"http\\S+|www\\S+\", \"\", text)    # remove links\n",
        "  text = re.sub(r\"[^a-z\\s]\", \"\", text)          # remove punctuation/numbers\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()      # remove extra spaces\n",
        "  return text\n",
        "\n",
        "df[\"clean_text\"] = df[\"title\"].astype(str) + \" \" + df[\"text\"].astype(str)\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(clean_text)\n",
        "\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2379e0a0"
      },
      "source": [
        "## Analyze data\n",
        "\n",
        "### Subtask:\n",
        "Perform some basic analysis on the collected data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eb0932"
      },
      "source": [
        "#calculating and display the average score and number of comments and find the highest score and most comments\n",
        "average_score = df['score'].mean()\n",
        "average_comments = df['comments'].mean()\n",
        "\n",
        "print(f\"Average Score: {average_score:.2f}\")\n",
        "print(f\"Average Comments: {average_comments:.2f}\")\n",
        "\n",
        "highest_score_post = df.loc[df['score'].idxmax()]\n",
        "most_comments_post = df.loc[df['comments'].idxmax()]\n",
        "\n",
        "print(\"\\nPost with Highest Score:\")\n",
        "display(highest_score_post)\n",
        "\n",
        "print(\"\\nPost with Most Comments:\")\n",
        "display(most_comments_post)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Word cloud\n",
        "text = \" \".join(df[\"clean_text\"].tolist())\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Post length distribution\n",
        "df[\"length\"] = df[\"clean_text\"].apply(len)\n",
        "df[\"length\"].hist(bins=30, figsize=(8,4))\n",
        "plt.title(\"Distribution of Post Lengths\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n5n8jYUJ9iA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Apply VADER\n",
        "df[\"vader_score\"] = df[\"clean_text\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
        "df[\"vader_sentiment\"] = df[\"vader_score\"].apply(\n",
        "    lambda x: \"positive\" if x > 0.05 else (\"negative\" if x < -0.05 else \"neutral\")\n",
        ")\n",
        "\n",
        "df[\"vader_sentiment\"].value_counts()\n"
      ],
      "metadata": {
        "id": "4ShSuqSf9wmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# Load FinBERT\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "finbert_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Apply FinBERT sentiment analysis\n",
        "def get_finbert_sentiment(text):\n",
        "    if not text.strip():\n",
        "        return \"neutral\"\n",
        "    result = finbert_pipeline(text[:512])[0]  # truncate long posts\n",
        "    return result[\"label\"]\n",
        "\n",
        "df[\"finbert_sentiment\"] = df[\"clean_text\"].apply(get_finbert_sentiment)\n",
        "df[\"finbert_sentiment\"].value_counts()\n"
      ],
      "metadata": {
        "id": "biCQGfWO-NeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"VADER\": df[\"vader_sentiment\"].value_counts(),\n",
        "    \"FinBERT\": df[\"finbert_sentiment\"].value_counts()\n",
        "}).fillna(0)\n",
        "\n",
        "comparison.plot(kind=\"bar\", figsize=(8,5))\n",
        "plt.title(\"Sentiment Distribution: VADER vs FinBERT\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5E-9eWM8EGD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"clean_text\", \"vader_sentiment\", \"finbert_sentiment\"]].head(20)\n"
      ],
      "metadata": {
        "id": "lKTj5OX1J3cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"VADER\": df[\"vader_sentiment\"].value_counts(),\n",
        "    \"FinBERT\": df[\"finbert_sentiment\"].value_counts()\n",
        "}).fillna(0)\n",
        "\n",
        "comparison.plot(kind=\"bar\", figsize=(8,5))\n",
        "plt.title(\"Sentiment Distribution: VADER vs FinBERT\")\n",
        "plt.show()\n",
        "\n",
        "comparison\n"
      ],
      "metadata": {
        "id": "I4YuAZYKJ6p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# Load tokenizer & model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "# Create sentiment pipeline\n",
        "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example text\n",
        "result = nlp(\"The stock price is expected to rise after earnings.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ZzLBTPrRL7-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataframe is df with a column 'cleaned_text'\n",
        "results = df[\"clean_text\"].apply(lambda x: nlp(x)[0]['label'])\n",
        "\n",
        "df[\"finbert_sentiment\"] = results\n",
        "print(df[\"finbert_sentiment\"].value_counts())"
      ],
      "metadata": {
        "id": "YsR83whATG4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch --quiet\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline"
      ],
      "metadata": {
        "id": "ygvbx-Y7UCye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model trained on 124M tweets\n",
        "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "# Load tokenizer & model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Build pipeline\n",
        "twitter_roberta = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "F4PXtlFk0DyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SAMPLE TEXT RUN\n",
        "example = \"TWITTER is going to the moon ðŸš€ðŸš€ðŸš€\"\n",
        "print(twitter_roberta(example))"
      ],
      "metadata": {
        "id": "9z-XGwtL09Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"GME is going to DOWN ,WHAT TO DO\"\n",
        "print(twitter_roberta(example))\n"
      ],
      "metadata": {
        "id": "A9jTSGAC1c3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sentiment analysis on cleaned text column\n",
        "df[\"twitter_roberta_sentiment\"] = df[\"clean_text\"].apply(lambda x: twitter_roberta(x)[0]['label'])\n",
        "\n",
        "# Count results\n",
        "print(df[\"twitter_roberta_sentiment\"].value_counts())"
      ],
      "metadata": {
        "id": "RE6c-JbL1uVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"twitter_roberta_sentiment\"].value_counts())"
      ],
      "metadata": {
        "id": "cbUMdfK62bY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot the distribution of Twitter RoBERTa sentiment labels\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=\"twitter_roberta_sentiment\", data=df, palette=\"viridis\")\n",
        "plt.title(\"Distribution of Twitter RoBERTa Sentiment Labels\")\n",
        "plt.xlabel(\"Sentiment Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CrISnBZD3Bzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c8IN5yd87ovJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}